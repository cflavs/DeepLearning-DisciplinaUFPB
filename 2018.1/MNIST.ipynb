{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O MNIST é uma base de imagens que contém 10 classes referentes a dígitos manuscritos. As imagens dessa base são binárias (preto e branco) e possuem dimensão 28x28, conforme Figura abaixo. Além disso, ao todo a base divide-se em 60.000 imagens de treino e 10.000 imagens de teste. O MNIST é uma base comumente utilizada para avaliar a performance de redes neurais, alcançando uma acurácia de 99.25% em arquiteturas simples de redes neurais convolucionais (CNNs). \n",
    "\n",
    "Dessa forma, esse notebook introduz como implementar CNNs no Keras utilizando o MNIST como base de imagens, tendo-se como passos principais:\n",
    "\n",
    "* Leitura e visualização da base\n",
    "* Adaptação da base para o modelo Keras \n",
    "* Criação do modelo da arquitetura no Keras \n",
    "* Treinamento\n",
    "* Teste\n",
    "\n",
    "![alt text](imgs/mninst.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura e Visualização da base\n",
    "\n",
    "\n",
    "O Keras possui a base do MNIST, para utilizá-la basta importar o dataset mnist e chamar a função load_data(). A função retornará separadamente a entrada e as classes para treinamento (x_train e y_train), assim como aa entrada e classes para o teste (x_test, y_test). Para visualizar a base, pode-se utilizar o matplotlib. É comum também, após a leitura, normalizar a base dividindo-a por 255. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### leitura do dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "### Visualizar instâncias\n",
    "plt.imshow(x_train[5], cmap=plt.get_cmap('gray'))\n",
    "plt.title(y_train[5])\n",
    "### Normalizar\n",
    "x_train = x_train/255\n",
    "x_test =  x_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptação da base para o modelo Keras \n",
    "\n",
    "O Keras utiliza as imagens de entrada no formato [quantidade de instancias da base, width, height, quantidade de canais] para o tensorflow, ao utilizar Theano no keras, as entradas devem ter o formato [quantidade de instancias da base, quantidade de canais, width, height]. Dessa forma, antes de criar o modelo, utiliza-se a função reshape para cada base de entrada individualmente (x_train e x_test). Nesse exemplo, a quantidade de instancias é igual a 60.000 para treino e 10.000 para teste, tais valores podem ser obtidos utilizando .shape[0].\n",
    "\n",
    "Além disso, os rótulos y_train e y_test devem ser convertidos para matrizes, uma vez que são tidos apenas como vetor inicialmente. Isso pode ser feito utilizando conversão one-hot encoding ou to_categorical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28)\n",
      "60000 Quantidade de instancias para treino\n",
      "10000 Quantidade de instancias para teste\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'Quantidade de instancias para treino')\n",
    "print(x_test.shape[0], 'Quantidade de instancias para teste')\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação do modelo CNN\n",
    "\n",
    "A arquitetura CNN é inicializada no Keras utilizando o método Sequential(), após isso, cada camada da rede será adicionada sequencialmente utilizando add. Dessa forma, para implementar a LeNet no MNIST, adiciona-se duas camadas convolucionais com quantidade de filtros 6 e 16 respectivamente, cada uma seguida por uma camada pooling. Por fim, a rede possui 3 camadas Dense (completamente conectadas) com 120, 84 e 10 neuronios cada.\n",
    "\n",
    "![alt text](imgs/LeNet-5-structure.png \"Title\")\n",
    "\n",
    "Além disso, em CNNs mais recentes comumente utilizam MaxPooling, função de ativação ReLU e técnicas de regularização como Dropout e normalização do Batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(6, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120, activation='relu'))\n",
    "model.add(Dense(84, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento\n",
    "\n",
    "Uma vez definida a arquitetura da rede, utiliza-se os métodos do Keras compile e fit para realizar o treinamento. O compile definirá métricas (e.g. acurácia) e otimizar que será utilizado. A escolha do otimizador varia de acordo com o problema e pode levar a diferentes resultados, conforme Figura abaixo.\n",
    "\n",
    "![alt text](imgs/otimizadores.gif \"Title\")\n",
    "\n",
    "Por fim, na função fit define-se o tamanho do batch, quantidade de epochs e os dados de validação da rede. O Keras permite obter a base de validação a partir da base de treino utilizando-se o parametro validation_split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 30s 628us/step - loss: 0.1702 - acc: 0.9481 - val_loss: 0.0673 - val_acc: 0.9797\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 30s 616us/step - loss: 0.0505 - acc: 0.9848 - val_loss: 0.0539 - val_acc: 0.9828\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 26s 542us/step - loss: 0.0307 - acc: 0.9901 - val_loss: 0.0473 - val_acc: 0.9852\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 28s 584us/step - loss: 0.0222 - acc: 0.9928 - val_loss: 0.0445 - val_acc: 0.9867\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 27s 556us/step - loss: 0.0160 - acc: 0.9946 - val_loss: 0.0538 - val_acc: 0.9850\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 28s 581us/step - loss: 0.0129 - acc: 0.9958 - val_loss: 0.0523 - val_acc: 0.9860\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 27s 556us/step - loss: 0.0102 - acc: 0.9964 - val_loss: 0.0488 - val_acc: 0.9876\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 27s 566us/step - loss: 0.0099 - acc: 0.9966 - val_loss: 0.0605 - val_acc: 0.9864\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 28s 589us/step - loss: 0.0099 - acc: 0.9965 - val_loss: 0.0563 - val_acc: 0.9866\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 32s 675us/step - loss: 0.0079 - acc: 0.9972 - val_loss: 0.0709 - val_acc: 0.9845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11d712ef0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.06170223154023606\n",
      "Test accuracy: 0.9833\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
